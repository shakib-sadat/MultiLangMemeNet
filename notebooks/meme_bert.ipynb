{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3OiYJvEUhEBy"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "import torch\n",
        "import random\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import accuracy_score\n",
        "from transformers import BertTokenizer\n",
        "from torch.utils.data import TensorDataset\n",
        "from transformers import BertForSequenceClassification\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "import plotly.graph_objects as go"
      ],
      "metadata": {
        "id": "8K5pOvJ4hUSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "import os\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import re,nltk,json\n",
        "from bs4 import BeautifulSoup\n",
        "### ML Librarires--------------------\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,roc_auc_score\n",
        "from sklearn.metrics import average_precision_score,roc_auc_score, roc_curve, precision_recall_curve\n",
        "###-------------------------------------------\n",
        "#from keras.utils.vis_utils import plot_model\n",
        "np.random.seed(42)\n",
        "import string, spacy,unicodedata, random\n",
        "class color: # Text style\n",
        "   PURPLE = '\\033[95m'\n",
        "   CYAN = '\\033[96m'\n",
        "   DARKCYAN = '\\033[36m'\n",
        "   BLUE = '\\033[94m'\n",
        "   GREEN = '\\033[92m'\n",
        "   YELLOW = '\\033[93m'\n",
        "   RED = '\\033[91m'\n",
        "   BOLD = '\\033[1m'\n",
        "   UNDERLINE = '\\033[4m'\n",
        "   END = '\\033[0m'\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "2aC_C1B3ieou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv('/content/drive/MyDrive/CM_MEMES-master/train_data.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/CM_MEMES-master/test_data.csv')\n",
        "\n",
        "print(\"Number of Training Data: \", len(train_data))\n",
        "print(\"Number of Test Data: \", len(test_data))"
      ],
      "metadata": {
        "id": "G-GW7pTkh39Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode sarcasm labels: 0 for sarcastic, 1 for non-sarcastic\n",
        "train_data['label'] = train_data['label'].apply(lambda x: 0 if x == 'sarcasm' else 1)\n",
        "test_data['label'] = test_data['label'].apply(lambda x: 0 if x == 'sarcasm' else 1)\n",
        "\n",
        "# Display the updated DataFrames\n",
        "print(\"Train data:\\n\", train_data.head())\n",
        "print(\"\\nTest data:\\n\", test_data.head())"
      ],
      "metadata": {
        "id": "y2hmp-0Xh8Yf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Text Cleaning\n",
        "'''\n",
        "def text_cleaning(row):\n",
        "   #to remove HTML tags\n",
        "  text = BeautifulSoup(row, 'html.parser').get_text()\n",
        "  d = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', text, flags=re.MULTILINE) #This line is for removing url\n",
        "  post = d.replace('\\n', '')\n",
        "  post = post.replace('â€”', ' ')\n",
        "  post = post.replace('à¥¤', ' ')\n",
        "  text = ''.join([c for c in post if c not in string.punctuation])\n",
        "  # to remove special characters\n",
        "  pattern = r'^\\s*|\\s\\s*'\n",
        "  text = re.sub(pattern, ' ', text).strip()\n",
        "  # convert into lower case\n",
        "  text = text.lower()\n",
        "  # # Stopword\n",
        "  # result = text.split()\n",
        "  # text = [word.strip() for word in result if word not in stp ]\n",
        "  # text =\" \".join(text)\n",
        "\n",
        "  return text"
      ],
      "metadata": {
        "id": "o-VLUEa9iPye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing punctuations\n",
        "train_data['cleaned'] = train_data['captions'].apply(text_cleaning)\n",
        "test_data['cleaned'] = test_data['captions'].apply(text_cleaning)"
      ],
      "metadata": {
        "id": "pwWe7GKvicm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.head()"
      ],
      "metadata": {
        "id": "6hB7kqd-in-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BERT**"
      ],
      "metadata": {
        "id": "fPwl-d8EiHpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    do_lower_case=True\n",
        ")"
      ],
      "metadata": {
        "id": "Lsv5OmbJiBO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_data_train = tokenizer.batch_encode_plus(\n",
        "    train_data.captions.values,\n",
        "    add_special_tokens=True, # set this so BERT knows when a sentence ends and begins\n",
        "    return_attention_mask=True, #using a fixed input, all sentences have same dimensionality. Tells us where actual values are and where zeros are\n",
        "    padding = True, # make sure to pad sentences so they are all the same length\n",
        "    max_length = 256, # this is the length we want all senteces to be\n",
        "    return_tensors = 'pt'\n",
        ")\n",
        "\n",
        "encoded_data_test = tokenizer.batch_encode_plus(\n",
        "    test_data.captions.values,\n",
        "    add_special_tokens=True, # set this so BERT knows when a sentence ends and begins\n",
        "    return_attention_mask=True, #using a fixed input, all sentences have same dimensionality. Tells us where actual values are and where zeros are\n",
        "    padding = True, # make sure to pad sentences so they are all the same length\n",
        "    max_length = 256, # this is the length we want all senteces to be\n",
        "    return_tensors = 'pt'\n",
        ")\n",
        "\n",
        "\n",
        "# get the parts from the encoding that need to train the model\n",
        "input_ids_train = encoded_data_train['input_ids']\n",
        "attention_masks_train = encoded_data_train['attention_mask']\n",
        "labels_train = torch.tensor(train_data.label.values)\n",
        "\n",
        "input_ids_test = encoded_data_test['input_ids']\n",
        "attention_masks_test = encoded_data_test['attention_mask']\n",
        "labels_test = torch.tensor(test_data.label.values)\n",
        "\n"
      ],
      "metadata": {
        "id": "tRPOde52iy1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train = TensorDataset(torch.tensor(input_ids_train),\n",
        "                              torch.tensor(attention_masks_train),\n",
        "                              labels_train)\n",
        "\n",
        "valid_dataset = TensorDataset(torch.tensor(input_ids_test),\n",
        "                              torch.tensor(attention_masks_test),\n",
        "                              labels_test)\n",
        "\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=8, shuffle=True)\n",
        "dataloader_test = DataLoader(valid_dataset, batch_size=8, shuffle=False)"
      ],
      "metadata": {
        "id": "DNC32xEzr1ZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    num_labels=len(train_data.label.unique()),\n",
        "    output_attentions=False,\n",
        "    output_hidden_states=False\n",
        ")"
      ],
      "metadata": {
        "id": "9FpG-U9vmJLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(\n",
        "    model.parameters(),\n",
        "    lr= 1e-5, #2e-5 > 5e -5\n",
        "    eps=1e-8\n",
        ")"
      ],
      "metadata": {
        "id": "Kn7Sh13Bmz-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 1\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps = 5 #len(dataloader_train) *epochs\n",
        ")"
      ],
      "metadata": {
        "id": "5_-6g-DAnNXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def auc(preds, labels):\n",
        "    preds_flat=np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat=labels.flatten()\n",
        "    return roc_auc_score(labels_flat, preds_flat)"
      ],
      "metadata": {
        "id": "qyomi5djnOj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_val = 17\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)"
      ],
      "metadata": {
        "id": "Gw_ROSmHni4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "print(device)"
      ],
      "metadata": {
        "id": "i1VZF-rOnkJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(dataloader_test):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    loss_val_total = 0\n",
        "    predictions, true_vals = [], []\n",
        "\n",
        "    for batch in tqdm(dataloader_test):\n",
        "\n",
        "        batch = tuple(b.to(device) for b in batch)\n",
        "\n",
        "        inputs = {'input_ids':      batch[0],\n",
        "                  'attention_mask': batch[1],\n",
        "                  'labels':         batch[2],\n",
        "                 }\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "        loss = outputs[0]\n",
        "        logits = outputs[1]\n",
        "        loss_val_total += loss.item()\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = inputs['labels'].cpu().numpy()\n",
        "        predictions.append(logits)\n",
        "        true_vals.append(label_ids)\n",
        "\n",
        "    loss_val_avg = loss_val_total/len(dataloader_test)\n",
        "\n",
        "    predictions = np.concatenate(predictions, axis=0)\n",
        "    true_vals = np.concatenate(true_vals, axis=0)\n",
        "\n",
        "    return loss_val_avg, predictions, true_vals"
      ],
      "metadata": {
        "id": "IXYCmfqZnnPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in tqdm(range(1, epochs+1)):\n",
        "    model.train() # put model in training mode\n",
        "\n",
        "    loss_train_total = 0\n",
        "\n",
        "    progress_bar = tqdm(dataloader_train,\n",
        "                        desc='Epoch {:1d}'.format(epoch),\n",
        "                       leave = False,\n",
        "                       disable = False)\n",
        "    for batch in progress_bar:\n",
        "\n",
        "        model.zero_grad() # set the gradient to zero\n",
        "\n",
        "        # Extract tensors from the batch (assuming batch is a TensorDataset)\n",
        "        input_ids = batch[0].to(device)\n",
        "        attention_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        # get the inputs to the model\n",
        "        inputs = {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask' : attention_mask,\n",
        "            'labels': labels\n",
        "        }\n",
        "        # get the outputs\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        #BERT returns loss and logits\n",
        "        loss = outputs[0]\n",
        "        loss_train_total += loss.item()\n",
        "        loss.backward() # backpropagation\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # prevents gradient from getting too small or too big\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
        "\n",
        "    torch.save(model.state_dict(), f'/content/drive/MyDrive/CM_MEMES-master/model/bertmodel.h5')\n",
        "    tqdm.write('\\nEpoch {epoch}')\n",
        "\n",
        "    loss_train_avg = loss_train_total/len(dataloader_train)\n",
        "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
        "\n",
        "    val_loss, predictions, true_value = evaluate(dataloader_test)\n",
        "    val_AUC = auc(predictions, true_value)\n",
        "    tqdm.write(f'Validation loss: {val_loss}')\n",
        "    tqdm.write(f'AUC Score: {val_AUC}')"
      ],
      "metadata": {
        "id": "3VN55Xn8n1_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
        "                                                      num_labels=len(train_data.label.unique()),\n",
        "                                                      output_attentions=False,\n",
        "                                                      output_hidden_states=False)\n",
        "model.to(device)\n",
        "pass"
      ],
      "metadata": {
        "id": "Dw3uq8gD0Nbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load a pretrained model\n",
        "model.load_state_dict(\n",
        "    torch.load('/content/drive/MyDrive/CM_MEMES-master/model/bertmodel.h5',\n",
        "    map_location = torch.device('cpu'))\n",
        ")"
      ],
      "metadata": {
        "id": "RnsAcS3a0O4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, predictions, true_vals = evaluate(dataloader_test)"
      ],
      "metadata": {
        "id": "hWSdYOV91UU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, train_predictions, train_true_vals = evaluate(dataloader_train)"
      ],
      "metadata": {
        "id": "4DHXDaew1g8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "#print(metrics.classification_report(predictions, true_vals))\n",
        "pred = predictions[:,1]\n",
        "pred = np.where(pred > 0, pred, 0)\n",
        "pred = np.where(pred <= 0, pred, 1)\n",
        "\n",
        "pred\n",
        "print(metrics.classification_report(pred, true_vals))"
      ],
      "metadata": {
        "id": "nYCrwnzP1mjw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}