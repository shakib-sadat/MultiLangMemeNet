{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKKuKeX_PNo0"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "import os\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import re,nltk,json\n",
        "from bs4 import BeautifulSoup\n",
        "### ML Librarires--------------------\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,roc_auc_score\n",
        "from sklearn.metrics import average_precision_score,roc_auc_score, roc_curve, precision_recall_curve\n",
        "###-------------------------------------------\n",
        "#from keras.utils.vis_utils import plot_model\n",
        "np.random.seed(42)\n",
        "import string, spacy,unicodedata, random\n",
        "class color: # Text style\n",
        "   PURPLE = '\\033[95m'\n",
        "   CYAN = '\\033[96m'\n",
        "   DARKCYAN = '\\033[36m'\n",
        "   BLUE = '\\033[94m'\n",
        "   GREEN = '\\033[92m'\n",
        "   YELLOW = '\\033[93m'\n",
        "   RED = '\\033[91m'\n",
        "   BOLD = '\\033[1m'\n",
        "   UNDERLINE = '\\033[4m'\n",
        "   END = '\\033[0m'\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "id": "8PMpS-uTU0-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras_preprocessing\n",
        "from keras.preprocessing import image\n",
        "from tensorflow.keras.applications.xception import Xception\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg19 import VGG19\n",
        "from keras.applications.densenet import DenseNet121\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.layers import Dense, GlobalAveragePooling2D, Embedding, LSTM, multiply\n",
        "from keras.models import Model\n",
        "from keras import preprocessing, Input\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "#Import from keras_preprocessing not from keras.preprocessing\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import regularizers, optimizers\n",
        "from tensorflow.keras.models import load_model\n",
        "import itertools\n",
        "from PIL import Image, ImageFile\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Concatenate, Dense, Input, LSTM, Embedding, Dropout, Activation, GRU, Flatten, Reshape, Permute\n",
        "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Convolution1D,MaxPooling1D,Conv1D\n",
        "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\n",
        "from tensorflow.keras.layers import Add, BatchNormalization, Activation, Dropout\n",
        "from tensorflow.keras.optimizers import RMSprop,Adam,SGD,Nadam"
      ],
      "metadata": {
        "id": "CQ8j1fYzZywl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# # Change the path to your actual data location\n",
        "# data_path = \"/content/drive/MyDrive/Dataset/multi-sent.xlsx\"\n",
        "\n",
        "# # Read the data into a Pandas DataFrame\n",
        "# data = pd.read_excel(data_path)\n",
        "\n",
        "# # Print the column names\n",
        "# print(data.columns)"
      ],
      "metadata": {
        "id": "6L2L9v8eaEak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Locate images from /content/drive/MyDrive/CM_MEMES-master/LREC_COLING_meme\n",
        "image_folder = \"/content/drive/MyDrive/Dataset/Memes\"\n",
        "image_names = set(os.listdir(image_folder))\n",
        "\n",
        "# Import sample_data.csv from /content/drive/MyDrive/CM_MEMES-master/sample_data.csv\n",
        "csv_file_path = \"/content/drive/MyDrive/Dataset/multi-sent.xlsx\"\n",
        "data = pd.read_excel(csv_file_path)\n",
        "\n",
        "# Cross-check all the image names with the Name column and remove rows that don't exist\n",
        "data = data[data[\"image_name\"].isin(image_names)]\n",
        "\n",
        "# Display the updated DataFrame\n",
        "print(data)"
      ],
      "metadata": {
        "id": "ak5J_Wc4l2Eq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "otg20HaHbFjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.rename(columns={\"Label_Sentiment\": \"label\"})"
      ],
      "metadata": {
        "id": "CAwQ15LSapKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomize data with seed\n",
        "data = data.sample(frac=1, random_state=42)\n",
        "\n",
        "# Split data into train and test sets (80:20)\n",
        "train_size = int(0.8 * len(data))\n",
        "train_data, test_data = data[:train_size], data[train_size:]\n",
        "\n",
        "print(\"Train data shape:\", train_data.shape)\n",
        "print(\"Test data shape:\", test_data.shape)"
      ],
      "metadata": {
        "id": "dmfiGWNAbONg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.to_csv('/content/drive/MyDrive/Dataset/train_data.csv', index=False)\n",
        "test_data.to_csv('/content/drive/MyDrive/Dataset/test_data.csv', index=False)"
      ],
      "metadata": {
        "id": "7VKCMZx4bj4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv('/content/drive/MyDrive/Dataset/train_data.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/Dataset/test_data.csv')\n",
        "\n",
        "print(\"Number of Training Data: \", len(train_data))\n",
        "print(\"Number of Test Data: \", len(test_data))"
      ],
      "metadata": {
        "id": "TFSGd-mPb0Ht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data['label'] = train_data['label'].replace({'positive':0,'negative':1,'neutral':2})\n",
        "test_data['label'] = test_data['label'].replace({'positive':0,'negative':1,'neutral':2})"
      ],
      "metadata": {
        "id": "Ham8OfxKiI2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memes_path = \"/content/drive/MyDrive/Dataset/Memes/\""
      ],
      "metadata": {
        "id": "c00Em8KhkPwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## collect image names from the folders\n",
        "def create_img_array(img_dirct):\n",
        "    all_imgs = []\n",
        "    for root, j, files in os.walk(img_dirct):\n",
        "        for file in files:\n",
        "            file = root + '' + file\n",
        "            all_imgs.append(file)\n",
        "    return all_imgs\n",
        "\n",
        "def create_img_path(DF, Col_name, img_dir):\n",
        "    img_path = [img_dir + '' + name for name in DF[Col_name]]\n",
        "    return img_path"
      ],
      "metadata": {
        "id": "FF0PZudPkZZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function that returns image reading from the path\n",
        "def get_input(path):\n",
        "    # Loading image from given path\n",
        "    # and resizing it to 150*150*3 format\n",
        "    ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "    img = tf.keras.utils.load_img(path, target_size=(150,150))\n",
        "    return(img)\n",
        "\n",
        "# Takes in image and preprocess it\n",
        "def process_input(img):\n",
        "    # Converting image to array\n",
        "    img_data = tf.keras.utils.img_to_array(img)\n",
        "    # Adding one more dimension to array\n",
        "    img_data = np.expand_dims(img_data, axis=0)\n",
        "    #\n",
        "    img_data = preprocess_input(img_data)\n",
        "    return(img_data)\n"
      ],
      "metadata": {
        "id": "0KHBO-mbkg7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_dir = memes_path\n",
        "# Creating train, test and validation image path\n",
        "train_img_path = create_img_path(train_data,'image_name', img_dir)\n",
        "test_img_path = create_img_path(test_data,'image_name', img_dir)"
      ],
      "metadata": {
        "id": "epdgx_vFki1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an array of training images\n",
        "train_images = []\n",
        "for i,m in enumerate(train_img_path):\n",
        "  input_img = get_input(m)\n",
        "  input_img = process_input(input_img)\n",
        "  train_images.append(input_img[0])\n",
        "  print(i)\n",
        "\n",
        "# convert into numpy array\n",
        "train_image = np.array(train_images)\n",
        "print(train_image.shape)"
      ],
      "metadata": {
        "id": "hLj4croMk2Et"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_path = '/content/drive/MyDrive/Dataset'"
      ],
      "metadata": {
        "id": "wBgB3oeqoFMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle as pkl\n",
        "with open(main_path+'train_image_new.pkl','wb') as f:\n",
        "    pkl.dump(train_image, f)"
      ],
      "metadata": {
        "id": "jO0QS2hfnxO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an array of test images\n",
        "test_images = []\n",
        "for i,m in enumerate(test_img_path):\n",
        "  input_img = get_input(m)\n",
        "  input_img = process_input(input_img)\n",
        "  test_images.append(input_img[0])\n",
        "  print(i)\n",
        "\n",
        "# convert into numpy array\n",
        "test_image = np.array(test_images)\n",
        "print(test_image.shape)"
      ],
      "metadata": {
        "id": "2HCjWCXMoPDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle as pkl\n",
        "with open(main_path+'test_image_new.pkl','wb') as f:\n",
        "    pkl.dump(test_image, f)"
      ],
      "metadata": {
        "id": "Ov7PmLn1ouJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle as pkl\n",
        "\n",
        "with open(main_path+'train_image_new.pkl','rb') as f:\n",
        "  train_image = pkl.load(f)\n",
        "  print(\"Training Images:-- \",train_image.shape)\n",
        "\n",
        "with open(main_path+'test_image_new.pkl','rb') as f:\n",
        "  test_image = pkl.load(f)\n",
        "  print(\"Test Images:-- \",test_image.shape)"
      ],
      "metadata": {
        "id": "3l6N8MBXo7Rv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Text Cleaning\n",
        "'''\n",
        "def text_cleaning(row):\n",
        "   #to remove HTML tags\n",
        "  text = BeautifulSoup(row, 'html.parser').get_text()\n",
        "  d = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', text, flags=re.MULTILINE) #This line is for removing url\n",
        "  post = d.replace('\\n', '')\n",
        "  post = post.replace('—', ' ')\n",
        "  post = post.replace('।', ' ')\n",
        "  text = ''.join([c for c in post if c not in string.punctuation])\n",
        "  # to remove special characters\n",
        "  pattern = r'^\\s*|\\s\\s*'\n",
        "  text = re.sub(pattern, ' ', text).strip()\n",
        "  # convert into lower case\n",
        "  text = text.lower()\n",
        "  # # Stopword\n",
        "  # result = text.split()\n",
        "  # text = [word.strip() for word in result if word not in stp ]\n",
        "  # text =\" \".join(text)\n",
        "\n",
        "  return text"
      ],
      "metadata": {
        "id": "GXt3hIe3paSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing punctuations\n",
        "train_data['cleaned'] = train_data['Captions'].apply(text_cleaning)\n",
        "test_data['cleaned'] = test_data['Captions'].apply(text_cleaning)\n",
        "\n",
        "\n",
        "## Data samples after cleaning\n",
        "print(\"Data samples after cleaning:\\n\")\n",
        "for i in range(100):\n",
        "  print(\"Original Data:===\\n\",train_data.Captions[i],\"\\nCleaned Data:===\\n\",train_data.cleaned[i],)"
      ],
      "metadata": {
        "id": "dFwNP8NJplN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.head()"
      ],
      "metadata": {
        "id": "xMknlCjDp4TU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_summary(dataset):\n",
        "    documents = []\n",
        "    words = []\n",
        "    u_words = []\n",
        "    class_label= [k for k,v in dataset.label.value_counts().to_dict().items()]\n",
        "  # find word list\n",
        "    for label in class_label:\n",
        "        word_list = [word.strip().lower() for t in list(dataset[dataset.label==label].cleaned) for word in t.strip().split()]\n",
        "        counts = dict()\n",
        "        for word in word_list:\n",
        "                counts[word] = counts.get(word, 0)+1\n",
        "    # sort the dictionary of word list\n",
        "        ordered = sorted(counts.items(), key= lambda item: item[1],reverse = False)\n",
        "    # Documents per class\n",
        "        documents.append(len(list(dataset[dataset.label==label].cleaned)))\n",
        "    # Total Word per class\n",
        "        words.append(len(word_list))\n",
        "    # Unique words per class\n",
        "        u_words.append(len(np.unique(word_list)))\n",
        "\n",
        "        print(\"\\nClass Name : \",label)\n",
        "        print(\"Number of Comments:{}\".format(len(list(dataset[dataset.label==label].cleaned))))\n",
        "        print(\"Number of Words:{}\".format(len(word_list)))\n",
        "        print(\"Number of Unique Words:{}\".format(len(np.unique(word_list))))\n",
        "        print(\"Most Frequent Words:\\n\")\n",
        "        for k,v in ordered[:50]:\n",
        "              print(\"{}\\t{}\".format(k,v))\n",
        "\n",
        "    return documents,words,u_words,class_label"
      ],
      "metadata": {
        "id": "SC-r-QBMpq2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents,words,u_words,class_names = data_summary(train_data)"
      ],
      "metadata": {
        "id": "pDeMmIotpvtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Eval**"
      ],
      "metadata": {
        "id": "Cvt-XOFuqidR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_metrices(true,pred):\n",
        "    print(confusion_matrix(true,pred))\n",
        "    print(classification_report(true,pred,target_names=['0','1']))\n",
        "    print(\"Accuracy : \",accuracy_score(true,pred))\n",
        "    print(\"Precison : \",precision_score(true,pred, average = 'weighted'))\n",
        "    print(\"Recall : \",recall_score(true,pred,  average = 'weighted'))\n",
        "    print(\"F1 : \",f1_score(true,pred,  average = 'weighted'))"
      ],
      "metadata": {
        "id": "51mGg__PqRML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models_path = \"/content/drive/MyDrive/Dataset/model/\""
      ],
      "metadata": {
        "id": "5gGRDGITqlhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Callbacks'''\n",
        "keras.backend.clear_session()\n",
        "def callbacks_check(model_name):\n",
        "  accuracy_threshold = 0.98\n",
        "\n",
        "  class myCallback(keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if(logs.get('accuracy')>accuracy_threshold):\n",
        "          print(\"\\nReached %2.2f%% accuracy so we will stop trianing\" % (accuracy_threshold*100))\n",
        "          self.model.stop_training = True\n",
        "\n",
        "  acc_callback = myCallback()\n",
        "  # Saved the Best Model\n",
        "  filepath = models_path+f\"{model_name}.h5\"\n",
        "  checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', verbose=2, save_best_only=True,\n",
        "                                             save_weights_only=False, mode='max')\n",
        "  # callback list\n",
        "  callback_list = [acc_callback, checkpoint]\n",
        "\n",
        "  return callback_list"
      ],
      "metadata": {
        "id": "H0Smq-puqwOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import compute_class_weight\n",
        "class_weights = compute_class_weight(\n",
        "                                        class_weight = \"balanced\",\n",
        "                                        classes = np.unique(train_data['label']),\n",
        "                                        y = train_data['label']\n",
        "                                    )\n",
        "class_weights = dict(zip(np.unique(train_data['label']), class_weights))\n",
        "class_weights"
      ],
      "metadata": {
        "id": "f9u_dSt4rx_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "03uWVnNLvSsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visual_models(pretrained_model):\n",
        "  '''\n",
        "      Input: Pretrained Models weight\n",
        "  '''\n",
        "  base_model = pretrained_model\n",
        "  base_model.trainable = False\n",
        "  y = base_model.output\n",
        "  pool = GlobalAveragePooling2D()(y)\n",
        "  #flatten = Flatten()(pool)\n",
        "  output = Dense(3, activation='softmax')(pool)\n",
        "  # train model\n",
        "  img_model = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "  return img_model"
      ],
      "metadata": {
        "id": "PWIdqBQLr1Nt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xception = Xception(weights='imagenet', include_top=False,input_shape=(150, 150, 3))\n",
        "inception = InceptionV3(weights='imagenet', include_top=False,input_shape=(150, 150, 3)) #added inception\n",
        "vgg19 = VGG19(weights='imagenet', include_top=False,input_shape=(150, 150, 3))\n",
        "vgg16 = VGG16(weights='imagenet', include_top=False,input_shape=(150, 150, 3))\n",
        "resnet = ResNet50(weights='imagenet', include_top=False,input_shape=(150, 150, 3))\n",
        "densenet = DenseNet121(weights='imagenet', include_top=False,input_shape=(150, 150, 3))"
      ],
      "metadata": {
        "id": "eksSCfeGr4xF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xcept_model = visual_models(xception)\n",
        "xcept_model.compile(loss='sparse_categorical_crossentropy',\n",
        "                    optimizer=Adam(),\n",
        "                    metrics = [\"accuracy\"])"
      ],
      "metadata": {
        "id": "d8jKydUor-V8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keras.backend.clear_session()"
      ],
      "metadata": {
        "id": "UbQV6imLx18A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xcept_model.fit(x=train_image,\n",
        "              y=train_data['label'],\n",
        "              epochs=20,\n",
        "              batch_size =32,\n",
        "              validation_split = 0.1,\n",
        "              verbose = 1,\n",
        "              #class_weight = weight,\n",
        "              callbacks = callbacks_check('xception')\n",
        "              )"
      ],
      "metadata": {
        "id": "zFrjJmlJsA70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Prediction\n",
        "model = load_model(models_path+\"xception.h5\")\n",
        "pred = model.predict(test_image)\n",
        "y_pred = np.argmax(pred,axis=1)"
      ],
      "metadata": {
        "id": "KZVcSMDusKcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_labels = y_true.unique()\n",
        "num_classes = len(unique_labels)"
      ],
      "metadata": {
        "id": "ngWdw3QfxnuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define target names based on the number of classes\n",
        "if num_classes == 3:\n",
        "    target_names = ['class_0', 'class_1', 'class_2']\n",
        "elif num_classes == 2:\n",
        "    target_names = ['class_0', 'class_1']\n",
        "else:\n",
        "    target_names = [f'class_{i}' for i in range(num_classes)]"
      ],
      "metadata": {
        "id": "OvmDvqI8xquJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(y_true, y_pred))\n",
        "print(classification_report(y_true, y_pred, target_names=target_names))"
      ],
      "metadata": {
        "id": "A6qTaURaxsv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inception_model = visual_models(inception)\n",
        "inception_model.compile(loss='sparse_categorical_crossentropy',\n",
        "                    optimizer=Adam(),\n",
        "                    metrics = [\"accuracy\"])"
      ],
      "metadata": {
        "id": "R4ohCPb8svqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keras.backend.clear_session()\n",
        "inception_model.fit(x=train_image,\n",
        "              y=train_data['label'],\n",
        "              epochs=20,\n",
        "              batch_size =32,\n",
        "              validation_split = 0.1,\n",
        "              verbose = 1,\n",
        "              callbacks = callbacks_check('inception')\n",
        "              )"
      ],
      "metadata": {
        "id": "K-qqi6rTsyJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prediction\n",
        "model = load_model(models_path+\"inception.h5\")\n",
        "pred = model.predict(test_image)\n",
        "y_pred = np.argmax(pred,axis=1)"
      ],
      "metadata": {
        "id": "-bTZC3WsszKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_labels = y_true.unique()\n",
        "num_classes = len(unique_labels)"
      ],
      "metadata": {
        "id": "AD09tEIWw8dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define target names based on the number of classes\n",
        "if num_classes == 3:\n",
        "    target_names = ['class_0', 'class_1', 'class_2']\n",
        "elif num_classes == 2:\n",
        "    target_names = ['class_0', 'class_1']\n",
        "else:\n",
        "    target_names = [f'class_{i}' for i in range(num_classes)]"
      ],
      "metadata": {
        "id": "_7IZeLdgw937"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(y_true, y_pred))\n",
        "print(classification_report(y_true, y_pred, target_names=target_names))"
      ],
      "metadata": {
        "id": "T9ho5BeDxEc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg19_model = visual_models(vgg19)\n",
        "vgg19_model.compile(loss='sparse_categorical_crossentropy',\n",
        "                    optimizer=Adam(),\n",
        "                    metrics = [\"accuracy\"])"
      ],
      "metadata": {
        "id": "BnwYngI0s3_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keras.backend.clear_session()\n",
        "vgg19_model.fit(x=train_image,\n",
        "              y=train_data['label'],\n",
        "              epochs=20,\n",
        "              batch_size =32,\n",
        "              validation_split = 0.1,\n",
        "              verbose = 1,\n",
        "              callbacks = callbacks_check('vgg19')\n",
        "              )"
      ],
      "metadata": {
        "id": "OL9bh1w0s50R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Prediction\n",
        "model = load_model(models_path+\"vgg19.h5\")\n",
        "pred = model.predict(test_image)\n",
        "y_pred = np.argmax(pred,axis=1)"
      ],
      "metadata": {
        "id": "4S21W3dgzD3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_labels = y_true.unique()\n",
        "num_classes = len(unique_labels)"
      ],
      "metadata": {
        "id": "njd654bIzKHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define target names based on the number of classes\n",
        "if num_classes == 3:\n",
        "    target_names = ['class_0', 'class_1', 'class_2']\n",
        "elif num_classes == 2:\n",
        "    target_names = ['class_0', 'class_1']\n",
        "else:\n",
        "    target_names = [f'class_{i}' for i in range(num_classes)]"
      ],
      "metadata": {
        "id": "_lr-hRK_zNSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(y_true, y_pred))\n",
        "print(classification_report(y_true, y_pred, target_names=target_names))"
      ],
      "metadata": {
        "id": "cYN92YDVzO66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keras.backend.clear_session()\n",
        "vgg16_model = visual_models(vgg16)\n",
        "vgg16_model.compile(loss='sparse_categorical_crossentropy',\n",
        "                    optimizer=RMSprop(),\n",
        "                    metrics = [\"accuracy\"])"
      ],
      "metadata": {
        "id": "lkkfeWGSzQHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg16_model.fit(x=train_image,\n",
        "              y=train_data['label'],\n",
        "              epochs=20,\n",
        "              batch_size =32,\n",
        "              validation_split = 0.1,\n",
        "              verbose = 1,\n",
        "              callbacks = callbacks_check('vgg16')\n",
        "              )"
      ],
      "metadata": {
        "id": "edA7r-RszkRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_labels = y_true.unique()\n",
        "num_classes = len(unique_labels)"
      ],
      "metadata": {
        "id": "WfvHof2zzotL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define target names based on the number of classes\n",
        "if num_classes == 3:\n",
        "    target_names = ['class_0', 'class_1', 'class_2']\n",
        "elif num_classes == 2:\n",
        "    target_names = ['class_0', 'class_1']\n",
        "else:\n",
        "    target_names = [f'class_{i}' for i in range(num_classes)]"
      ],
      "metadata": {
        "id": "AUBd4b2RzrG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(y_true, y_pred))\n",
        "print(classification_report(y_true, y_pred, target_names=target_names))"
      ],
      "metadata": {
        "id": "j4lZ_L_szsap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keras.backend.clear_session()\n",
        "resnet_model = visual_models(resnet)\n",
        "resnet_model.compile(loss='sparse_categorical_crossentropy',\n",
        "                    optimizer=RMSprop(),\n",
        "                    metrics = [\"accuracy\"])"
      ],
      "metadata": {
        "id": "xaFtHpOmztgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_model.fit(x=train_image,\n",
        "              y=train_data['label'],\n",
        "              epochs=20,\n",
        "              batch_size =32,\n",
        "              validation_split = 0.1,\n",
        "              verbose = 1,\n",
        "              callbacks = callbacks_check('resnet-1')\n",
        "              )"
      ],
      "metadata": {
        "id": "GvMsrP5g0BOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Prediction\n",
        "model = load_model(models_path+\"resnet.h5\")\n",
        "pred = model.predict(test_image)\n",
        "y_pred = np.argmax(pred,axis=1)"
      ],
      "metadata": {
        "id": "3nSZfAWO0DIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_labels = y_true.unique()\n",
        "num_classes = len(unique_labels)"
      ],
      "metadata": {
        "id": "DgTDOYnj0Esq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define target names based on the number of classes\n",
        "if num_classes == 3:\n",
        "    target_names = ['class_0', 'class_1', 'class_2']\n",
        "elif num_classes == 2:\n",
        "    target_names = ['class_0', 'class_1']\n",
        "else:\n",
        "    target_names = [f'class_{i}' for i in range(num_classes)]"
      ],
      "metadata": {
        "id": "lnnNMcuT0GAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(y_true, y_pred))\n",
        "print(classification_report(y_true, y_pred, target_names=target_names))"
      ],
      "metadata": {
        "id": "YwZf7_sl0JaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keras.backend.clear_session()\n",
        "densenet_model = visual_models(densenet)\n",
        "densenet_model.compile(loss='sparse_categorical_crossentropy',\n",
        "                    optimizer=Adam(),\n",
        "                    metrics = [\"accuracy\"])"
      ],
      "metadata": {
        "id": "SRoa_uXa0RTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "densenet_model.fit(x=train_image,\n",
        "              y=train_data['label'],\n",
        "              epochs=20,\n",
        "              batch_size =32,\n",
        "              validation_split = 0.1,\n",
        "              verbose = 1,\n",
        "              callbacks = callbacks_check('densenet')\n",
        "              )"
      ],
      "metadata": {
        "id": "t61I_Gff0T0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Prediction\n",
        "model = load_model(models_path+\"densenet.h5\")\n",
        "pred = model.predict(test_image)\n",
        "y_pred = np.argmax(pred,axis=1)"
      ],
      "metadata": {
        "id": "GEEyWmbL0Xh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_labels = y_true.unique()\n",
        "num_classes = len(unique_labels)"
      ],
      "metadata": {
        "id": "0cxKpBOH0Ymp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define target names based on the number of classes\n",
        "if num_classes == 3:\n",
        "    target_names = ['class_0', 'class_1', 'class_2']\n",
        "elif num_classes == 2:\n",
        "    target_names = ['class_0', 'class_1']\n",
        "else:\n",
        "    target_names = [f'class_{i}' for i in range(num_classes)]"
      ],
      "metadata": {
        "id": "sQffSeG10bcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(y_true, y_pred))\n",
        "print(classification_report(y_true, y_pred, target_names=target_names))"
      ],
      "metadata": {
        "id": "AnroVPkE0cx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visual_models = ['xception','inception','vgg19','vgg16','resnet','densenet']\n",
        "visual_model_names = ['Xception','Inception','vgg19','Vgg16','Resnet','Densenet']\n",
        "\n",
        "def visual_models_accuracy(saved_model):\n",
        "  my_dict = {}\n",
        "  # Prediction\n",
        "  model = load_model(models_path+f\"{saved_model}.h5\")\n",
        "  pred = model.predict(test_image)\n",
        "  y_pred = np.argmax(pred,axis=1)\n",
        "\n",
        "  y_true = test_data['label']\n",
        "\n",
        "  my_dict['Accuracy'] = accuracy_score(y_true, y_pred)*100\n",
        "  my_dict['Precision'] = precision_score(y_true, y_pred,average = 'weighted')*100\n",
        "  my_dict['Recall'] = recall_score(y_true, y_pred,average = 'weighted')*100\n",
        "  my_dict['F1 Score'] = f1_score(y_true, y_pred,average = 'weighted')*100\n",
        "  return my_dict"
      ],
      "metadata": {
        "id": "kYb68dGl1XU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_path = '/content/drive/MyDrive/Dataset/results/'\n",
        "\n",
        "accuracy = {f'{visual_model_names[i]}':visual_models_accuracy(model) for i,model in enumerate(visual_models)}\n",
        "# Save the performance parameter into json file\n",
        "with open(results_path+'visual_models_performance.json', 'w') as f:\n",
        "    json.dump(accuracy, f)"
      ],
      "metadata": {
        "id": "C34qflvv1lV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the json file\n",
        "metrics = json.load(open(results_path+'visual_models_performance.json'))\n",
        "acc_list = []\n",
        "pr_list = []\n",
        "re_list = []\n",
        "f1_list = []\n",
        "for i in metrics.keys():\n",
        "  acc_list.append(round(metrics[i]['Accuracy'],2))\n",
        "  pr_list.append(round(metrics[i]['Precision'],2))\n",
        "  re_list.append(round(metrics[i]['Recall'],2))\n",
        "  f1_list.append(round(metrics[i]['F1 Score'],2))\n",
        "\n",
        "print (color.BOLD+f\"=======  Visual Models Performance on Test Data  =============\\n\"+color.END)\n",
        "# Create a dataframe\n",
        "performance_matrix = pd.DataFrame({'Accuracy':acc_list,'Precision':pr_list,\n",
        "                                   'Recall':re_list,'F1 Score':f1_list},\n",
        "                                  index =['Xception', 'Inception','Vgg19','Vgg16','Resnet','Densenet'])\n",
        "performance_matrix"
      ],
      "metadata": {
        "id": "IXmYsweu3H8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text**"
      ],
      "metadata": {
        "id": "qRXVh3Tp4m-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Text Feature Extraction using TF-IDF\n",
        "cv = TfidfVectorizer(use_idf=True,tokenizer=lambda x: x.split())\n",
        "train_X = cv.fit_transform(train_data.Captions)\n",
        "\n",
        "# Test data\n",
        "test_X = cv.transform(test_data.Captions)\n",
        "\n",
        "print(\"\\nShape of TF-IDF Corpus =====>\",train_X.shape)\n",
        "print(\"\\nShape of TF-IDF Corpus =====>\",test_X.shape)"
      ],
      "metadata": {
        "id": "j6WjvXT03PlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_model = LogisticRegression(random_state = 123)\n",
        "dt_model = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
        "rf_model = RandomForestClassifier(n_estimators=10, criterion ='entropy', random_state = 0)\n",
        "mnb_model = MultinomialNB(alpha=0.05)\n",
        "svm_model = SVC(kernel = 'linear',probability=True,random_state = 0)\n",
        "model_names = ['Logistic Regression','Naive Bayes','SVM']\n",
        "ml_models = [lr_model,mnb_model,svm_model]\n",
        "\n",
        "def model_accuracy(model,X_train,X_test,y_train,y_test):\n",
        "  my_dict = {}\n",
        "  model.fit(X_train,y_train)\n",
        "  # Prediction\n",
        "  pred_y = model.predict(X_test)\n",
        "  my_dict['Accuracy'] = round(accuracy_score(y_test, pred_y),3)*100\n",
        "  my_dict['Precision'] = round(precision_score(y_test, pred_y,average='weighted'),3)*100\n",
        "  my_dict['Recall'] = round(recall_score(y_test, pred_y,average='weighted'),3)*100\n",
        "  my_dict['F1 Score'] = round(f1_score(y_test, pred_y,average='weighted'),3)*100\n",
        "  return my_dict\n",
        "\n",
        "# call model accuracy function and save the metrices into a dictionary\n",
        "accuracy = {f'{model_names[i]}':model_accuracy(model,train_X,test_X,train_data['label'],test_data['label']) for i,model in enumerate(ml_models)}\n",
        "# Save the performance parameter into json file\n",
        "with open(results_path +'ml_model_performance.json', 'w') as f:\n",
        "    json.dump(accuracy, f)"
      ],
      "metadata": {
        "id": "sr89F3iW5GSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the json file\n",
        "accuracy = json.load(open(results_path +'ml_model_performance.json'))\n",
        "acc_list = []\n",
        "pr_list = []\n",
        "re_list = []\n",
        "f1_list = []\n",
        "for i in accuracy.keys():\n",
        "  acc_list.append(accuracy[i]['Accuracy'])\n",
        "  pr_list.append(accuracy[i]['Precision'])\n",
        "  re_list.append(accuracy[i]['Recall'])\n",
        "  f1_list.append(accuracy[i]['F1 Score'])\n",
        "\n",
        "# Create a dataframe\n",
        "performance_matrix = pd.DataFrame({'Accuracy':acc_list,'Precision':pr_list,\n",
        "                                   'Recall':re_list,'F1 Score':f1_list},\n",
        "                                  index =['LR','MNB','SVM'])\n",
        "performance_matrix"
      ],
      "metadata": {
        "id": "xAd_e0C155Tc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_tokenizer(train_data,test_data,vocabulary,max_len,sample_text_num):\n",
        "\n",
        "  tokenizer = Tokenizer(num_words = vocabulary ,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n-',\n",
        "                        split=' ', char_level=False, oov_token='<oov>', document_count=0)\n",
        "  tokenizer.fit_on_texts(train_data['Captions'])\n",
        "  word_index = tokenizer.word_index\n",
        "  vocab_size = len(word_index)+1\n",
        "\n",
        "  # Training Sequences\n",
        "  train_sequences = tokenizer.texts_to_sequences(train_data['Captions'])\n",
        "  train_pad_sequences =  keras.preprocessing.sequence.pad_sequences(train_sequences, value=0.0, padding='post', maxlen= max_len)\n",
        "\n",
        "  # valid Sequences\n",
        "  # valid_sequences = tokenizer.texts_to_sequences(valid_data['Captions'])\n",
        "  # valid_pad_sequences =  keras.preprocessing.sequence.pad_sequences(valid_sequences, value=0.0, padding='post', maxlen= max_len)\n",
        "\n",
        "\n",
        "  # Test Sequences\n",
        "  test_sequences = tokenizer.texts_to_sequences(test_data['Captions'])\n",
        "  test_pad_sequences =  keras.preprocessing.sequence.pad_sequences(test_sequences, value=0.0, padding='post', maxlen= max_len)\n",
        "\n",
        "\n",
        "  print(color.BOLD+\"\\n\\t\\t\\t====== Encoded Sequences ======\"+color.END,\"\\n\")\n",
        "  print(train_data.Captions[sample_text_num],\"\\n\",train_sequences[sample_text_num])\n",
        "  print(color.BOLD+\"\\n\\t\\t\\t====== Paded Sequences ======\\n\"+color.END,\"\\n\",train_pad_sequences[sample_text_num])\n",
        "\n",
        "  return train_pad_sequences,  test_pad_sequences, vocab_size, word_index\n",
        "\n",
        "  # return train_pad_sequences, valid_pad_sequences, test_pad_sequences, vocab_size, word_index\n",
        "\n",
        "\n",
        "vocabulary = 15000\n",
        "max_len = 60\n",
        "sample_text_num = 10\n",
        "\n",
        "## Call Tokenizer\n",
        "# train_pad_sequences, valid_pad_sequences, test_pad_sequences, vocab_size, word_index =  text_tokenizer(train_data,test_data,\n",
        "#                                                                       vocabulary,max_len,sample_text_num)\n",
        "\n",
        "train_pad_sequences,  test_pad_sequences, vocab_size, word_index =  text_tokenizer(train_data,test_data,\n",
        "                                                                      vocabulary,max_len,sample_text_num)\n",
        "\n",
        "print(\"Number of Train Sequences :\" ,train_pad_sequences.shape)\n",
        "# print(\"Number of Test Sequences :\" ,valid_pad_sequences.shape)\n",
        "print(\"Number of Test Sequences :\" ,test_pad_sequences.shape)\n",
        "print(\"Vocabulary Size: \",vocab_size)"
      ],
      "metadata": {
        "id": "kNkCA8h56BLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###### BiLSTM Model #######\n",
        "bi_text_inputs = Input(shape=(max_len,))\n",
        "bi_embedding_layer = Embedding(vocab_size,64)(bi_text_inputs)\n",
        "LSTM_Layer_1 = Bidirectional(LSTM(32))(bi_embedding_layer)\n",
        "bi_dense_layer_1 = Dense(3, activation='softmax')(LSTM_Layer_1)\n",
        "bilstm_model = Model(inputs=bi_text_inputs, outputs=bi_dense_layer_1)"
      ],
      "metadata": {
        "id": "auHb1Op_6N6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keras.backend.clear_session()\n",
        "bilstm_model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
        "\n",
        "\n",
        "bilstm_model.fit(train_pad_sequences,\n",
        "    train_data['label'],\n",
        "    epochs= 20,\n",
        "    batch_size =32,\n",
        "    validation_split = 0.1,\n",
        "    verbose =1,\n",
        "    callbacks = callbacks_check('lstm'))"
      ],
      "metadata": {
        "id": "HXEK6xZq6QkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Prediction\n",
        "model = load_model(models_path + \"lstm.h5\")\n",
        "pred = model.predict(test_pad_sequences)\n",
        "y_pred = np.argmax(pred,axis=1)"
      ],
      "metadata": {
        "id": "reVDDP5c6VZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_labels = y_true.unique()\n",
        "num_classes = len(unique_labels)\n",
        "# Define target names based on the number of classes\n",
        "if num_classes == 3:\n",
        "    target_names = ['class_0', 'class_1', 'class_2']\n",
        "elif num_classes == 2:\n",
        "    target_names = ['class_0', 'class_1']\n",
        "else:\n",
        "    target_names = [f'class_{i}' for i in range(num_classes)]\n",
        "print(confusion_matrix(y_true, y_pred))\n",
        "print(classification_report(y_true, y_pred, target_names=target_names))"
      ],
      "metadata": {
        "id": "EIY16vd96k1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### CNN\n",
        "cnn_text_inputs = Input(shape=(max_len,))\n",
        "cnn_embedding_layer = Embedding(vocab_size,64)(cnn_text_inputs)\n",
        "cnn_conv1 = Conv1D(64,2,activation='relu')(cnn_embedding_layer)\n",
        "cnn_conv2 = Conv1D(32,2,activation='relu')(cnn_conv1)\n",
        "cnn_pool1 = MaxPooling1D(2)(cnn_conv2)\n",
        "\n",
        "cnn_flat = Flatten()(cnn_pool1)\n",
        "cnn_dense_layer_1 = Dense(3, activation='softmax')(cnn_flat)\n",
        "cnn_model = Model(inputs=cnn_text_inputs, outputs=cnn_dense_layer_1)"
      ],
      "metadata": {
        "id": "2UPci6VK6yNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keras.backend.clear_session()\n",
        "cnn_model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
        "\n",
        "\n",
        "cnn_model.fit(train_pad_sequences,\n",
        "    train_data['label'],\n",
        "    epochs=20,\n",
        "    batch_size =32,\n",
        "    validation_split = 0.1,\n",
        "    verbose =1,\n",
        "    callbacks = callbacks_check('cnn'))"
      ],
      "metadata": {
        "id": "9NF_5GGs63q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Prediction\n",
        "model = load_model(models_path+\"cnn.h5\")\n",
        "pred = model.predict(test_pad_sequences)\n",
        "y_pred = np.argmax(pred,axis=1)\n",
        "unique_labels = y_true.unique()\n",
        "num_classes = len(unique_labels)\n",
        "# Define target names based on the number of classes\n",
        "if num_classes == 3:\n",
        "    target_names = ['class_0', 'class_1', 'class_2']\n",
        "elif num_classes == 2:\n",
        "    target_names = ['class_0', 'class_1']\n",
        "else:\n",
        "    target_names = [f'class_{i}' for i in range(num_classes)]\n",
        "print(confusion_matrix(y_true, y_pred))\n",
        "print(classification_report(y_true, y_pred, target_names=target_names))"
      ],
      "metadata": {
        "id": "b5Ttq1tJ66eQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### LSTM-CNN Model ####\n",
        "lc_text_inputs = Input(shape=(max_len,))\n",
        "lc_embedding_layer = Embedding(vocab_size,64)(lc_text_inputs)\n",
        "LSTM_Layer = Bidirectional(LSTM(32,return_sequences=True))(lc_embedding_layer)\n",
        "lc_conv1 = Conv1D(32,2,activation='relu')(LSTM_Layer)\n",
        "lc_pool1 = MaxPooling1D(2)(lc_conv1)\n",
        "lc_flat = Flatten()(lc_pool1)\n",
        "lc_dense_layer_1 = Dense(3, activation='softmax')(lc_flat)\n",
        "cnn_lstm_model = Model(inputs=lc_text_inputs, outputs=lc_dense_layer_1)"
      ],
      "metadata": {
        "id": "7n2Aa6Io7Drf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keras.backend.clear_session()\n",
        "cnn_lstm_model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
        "\n",
        "\n",
        "cnn_lstm_model.fit(train_pad_sequences,\n",
        "    train_data['label'],\n",
        "    epochs=20,\n",
        "    batch_size =32,\n",
        "    validation_split = 0.1,\n",
        "    verbose =1,\n",
        "    callbacks = callbacks_check('lstm-cnn'))"
      ],
      "metadata": {
        "id": "3MK9eSlM7Gqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Prediction\n",
        "model = load_model(models_path+\"cnn.h5\")\n",
        "pred = model.predict(test_pad_sequences)\n",
        "y_pred = np.argmax(pred,axis=1)\n",
        "unique_labels = y_true.unique()\n",
        "num_classes = len(unique_labels)\n",
        "# Define target names based on the number of classes\n",
        "if num_classes == 3:\n",
        "    target_names = ['class_0', 'class_1', 'class_2']\n",
        "elif num_classes == 2:\n",
        "    target_names = ['class_0', 'class_1']\n",
        "else:\n",
        "    target_names = [f'class_{i}' for i in range(num_classes)]\n",
        "print(confusion_matrix(y_true, y_pred))\n",
        "print(classification_report(y_true, y_pred, target_names=target_names))"
      ],
      "metadata": {
        "id": "wSSGj0957JxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multimodal (Late Fusion)**"
      ],
      "metadata": {
        "id": "KJWA_yXa7L9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visual_models():\n",
        "  '''\n",
        "      Input: Pretrained Models weight\n",
        "  '''\n",
        "  ## ResNet50\n",
        "  resnet = ResNet50(weights='imagenet', include_top=False,input_shape=(150, 150, 3))\n",
        "  resnet_model = resnet\n",
        "  resnet_model.trainable = False\n",
        "  resnet_y = resnet_model.output\n",
        "  resnet_pool = GlobalAveragePooling2D()(resnet_y)\n",
        "  resnet_output = Dense(2, activation='softmax')(resnet_pool)\n",
        "  # this is the model we will train\n",
        "  resnet_img_model = Model(inputs=resnet_model.input, outputs=resnet_output)\n",
        "\n",
        "  ## VGG16\n",
        "  vgg16 = VGG16(weights='imagenet', include_top=False,input_shape=(150, 150, 3))\n",
        "  vgg16_model = vgg16\n",
        "  vgg16_model.trainable = False\n",
        "  vgg16_y = vgg16_model.output\n",
        "  vgg16_pool = GlobalAveragePooling2D()(vgg16_y)\n",
        "  vgg16_output = Dense(2, activation='softmax')(vgg16_pool)\n",
        "  # this is the model we will train\n",
        "  vgg16_img_model = Model(inputs=vgg16_model.input, outputs=vgg16_output)\n",
        "\n",
        "  ## DenseNet\n",
        "  densenet = DenseNet121(weights='imagenet', include_top=False,input_shape=(150, 150, 3))\n",
        "  densenet_model = densenet\n",
        "  densenet_model.trainable = False\n",
        "  densenet_y = densenet_model.output\n",
        "  densenet_pool = GlobalAveragePooling2D()(densenet_y)\n",
        "  densenet_output = Dense(2, activation='softmax')(densenet_pool)\n",
        "  # this is the model we will train\n",
        "  densenet_img_model = Model(inputs=densenet_model.input, outputs=densenet_output)\n",
        "\n",
        "  ## Inception V3\n",
        "  inception = InceptionV3(weights='imagenet', include_top=False,input_shape=(150, 150, 3))\n",
        "  inception_model = inception\n",
        "  inception_model.trainable = False\n",
        "  inception_y = inception_model.output\n",
        "  inception_pool = GlobalAveragePooling2D()(inception_y)\n",
        "  inception_output = Dense(2, activation='softmax')(inception_pool)\n",
        "  # this is the model we will train\n",
        "  inception_img_model = Model(inputs=inception_model.input, outputs=inception_output)\n",
        "\n",
        "  image_models = [resnet_img_model,densenet_img_model,vgg16_img_model,inception_model]\n",
        "\n",
        "  return image_models\n",
        "\n",
        "\n",
        "def textual_models():\n",
        "\n",
        "  ###### BiLSTM Model #######\n",
        "  bi_text_inputs = Input(shape=(max_len,))\n",
        "  bi_embedding_layer = Embedding(vocab_size,64)(bi_text_inputs)\n",
        "  LSTM_Layer_1 = Bidirectional(LSTM(32))(bi_embedding_layer)\n",
        "  bi_dense_layer_1 = Dense(3, activation='softmax')(LSTM_Layer_1)\n",
        "  bilstm_model = Model(inputs=bi_text_inputs, outputs=bi_dense_layer_1)\n",
        "\n",
        "\n",
        "  ### CNN\n",
        "  cnn_text_inputs = Input(shape=(max_len,))\n",
        "  cnn_embedding_layer = Embedding(vocab_size,64)(cnn_text_inputs)\n",
        "  cnn_conv1 = Conv1D(64,2,activation='relu')(cnn_embedding_layer)\n",
        "  cnn_conv2 = Conv1D(32,2,activation='relu')(cnn_conv1)\n",
        "  cnn_pool1 = MaxPooling1D(2)(cnn_conv2)\n",
        "\n",
        "  cnn_flat = Flatten()(cnn_pool1)\n",
        "  cnn_dense_layer_1 = Dense(3, activation='softmax')(cnn_flat)\n",
        "  cnn_model = Model(inputs=cnn_text_inputs, outputs=cnn_dense_layer_1)\n",
        "\n",
        "\n",
        "\n",
        "  ##### LSTM-CNN Model ####\n",
        "  lc_text_inputs = Input(shape=(max_len,))\n",
        "  lc_embedding_layer = Embedding(vocab_size,64)(lc_text_inputs)\n",
        "  LSTM_Layer = Bidirectional(LSTM(32,return_sequences=True))(lc_embedding_layer)\n",
        "  lc_conv1 = Conv1D(32,2,activation='relu')(LSTM_Layer)\n",
        "  lc_pool1 = MaxPooling1D(2)(lc_conv1)\n",
        "  lc_flat = Flatten()(lc_pool1)\n",
        "  lc_dense_layer_1 = Dense(3, activation='softmax')(lc_flat)\n",
        "  cnn_lstm_model = Model(inputs=lc_text_inputs, outputs=lc_dense_layer_1)\n",
        "\n",
        "  ############\n",
        "  dl_textual_models = [bilstm_model,cnn_model,cnn_lstm_model]\n",
        "\n",
        "  return dl_textual_models"
      ],
      "metadata": {
        "id": "cnhraEWK7N2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visual_model_name = ['ResNet','DenseNet','Vgg16','Inception']\n",
        "visual_models_lsit = visual_models()\n",
        "\n",
        "dl_textual_model_name = ['LSTM','CNN','LSTM+CNN']\n",
        "dl_textual_models_lsit = textual_models()"
      ],
      "metadata": {
        "id": "6NiIf7ts7S6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resnet"
      ],
      "metadata": {
        "id": "b2Tabnf-7U7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for tm,textual_model in enumerate(dl_textual_models_lsit):\n",
        "  # Concatenating the output of 2 classifiers\n",
        "  con_layer = keras.layers.concatenate([visual_models_lsit[0].output, textual_model.output])\n",
        "  final_dense = Dense(4, activation=\"relu\")(con_layer)\n",
        "  #dropout = Dropout(0.1)(final_dense)\n",
        "  out = Dense(3,activation='softmax')(final_dense)\n",
        "  com_model = Model(inputs = [visual_models_lsit[0].input, textual_model.input], outputs=out)\n",
        "\n",
        "  com_model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(), metrics = [\"accuracy\"])\n",
        "\n",
        "\n",
        "  print(\"\\nModel Name:=====>\\n\",visual_model_name[0]+'_'+dl_textual_model_name[tm]+'_late')\n",
        "\n",
        "  com_model.fit([train_image,train_pad_sequences],\n",
        "                      train_data['label'],\n",
        "                      validation_split = 0.1,\n",
        "                      epochs=20,\n",
        "                      batch_size=32,\n",
        "                      callbacks = callbacks_check(visual_model_name[0]+'_'+dl_textual_model_name[tm]+'_late')\n",
        "            )"
      ],
      "metadata": {
        "id": "A3G4-dzJ91hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for tm,textual_model in enumerate(dl_textual_models_lsit):\n",
        "  # Concatenating the output of 2 classifiers\n",
        "  con_layer = keras.layers.concatenate([visual_models_lsit[0].output, textual_model.output])\n",
        "  # Provide a unique name for this Dense layer using tm in the name\n",
        "  final_dense = Dense(4, activation=\"relu\", name=f\"final_dense_{tm}\")(con_layer)\n",
        "  #dropout = Dropout(0.1)(final_dense)\n",
        "  # Provide a unique name for the output layer using tm in the name\n",
        "  out = Dense(3,activation='softmax', name=f\"output_layer_{tm}\")(final_dense)\n",
        "  com_model = Model(inputs = [visual_models_lsit[0].input, textual_model.input], outputs=out)\n",
        "\n",
        "  com_model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(), metrics = [\"accuracy\"])\n",
        "\n",
        "\n",
        "  print(\"\\nModel Name:=====>\\n\",visual_model_name[0]+'_'+dl_textual_model_name[tm]+'_late')\n",
        "\n",
        "  com_model.fit([train_image,train_pad_sequences],\n",
        "                      train_data['label'],\n",
        "                      validation_split = 0.1,\n",
        "                      epochs=20,\n",
        "                      batch_size=32,\n",
        "                      callbacks = callbacks_check(visual_model_name[0]+'_'+dl_textual_model_name[tm]+'_late')\n",
        "            )"
      ],
      "metadata": {
        "id": "nKimZGOd7UH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Densenet"
      ],
      "metadata": {
        "id": "wmBwICCy7daQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for tm,textual_model in enumerate(dl_textual_models_lsit):\n",
        "  # Concatenating the output of 2 classifiers\n",
        "  con_layer = keras.layers.concatenate([visual_models_lsit[1].output, textual_model.output])\n",
        "  # Provide a unique name for this Dense layer using tm in the name\n",
        "  final_dense = Dense(4, activation=\"relu\", name=f\"final_dense_{tm}\")(con_layer)\n",
        "  #dropout = Dropout(0.1)(final_dense)\n",
        "  # Provide a unique name for the output layer using tm in the name\n",
        "  out = Dense(3,activation='softmax', name=f\"output_layer_{tm}\")(final_dense)\n",
        "  com_model = Model(inputs = [visual_models_lsit[1].input, textual_model.input], outputs=out)\n",
        "\n",
        "  com_model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(), metrics = [\"accuracy\"])\n",
        "\n",
        "\n",
        "  print(\"\\nModel Name:=====>\\n\",visual_model_name[1]+'_'+dl_textual_model_name[tm]+'_late')\n",
        "\n",
        "  com_model.fit([train_image,train_pad_sequences],\n",
        "                      train_data['label'],\n",
        "                      validation_split = 0.1,\n",
        "                      epochs=20,\n",
        "                      batch_size=32,\n",
        "                      callbacks = callbacks_check(visual_model_name[1]+'_'+dl_textual_model_name[tm]+'_late')\n",
        "            )"
      ],
      "metadata": {
        "id": "hJCIPDGm7aWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vgg"
      ],
      "metadata": {
        "id": "3D58wjd-7jsR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for tm,textual_model in enumerate(dl_textual_models_lsit):\n",
        "  # Concatenating the output of 2 classifiers\n",
        "  con_layer = keras.layers.concatenate([visual_models_lsit[2].output, textual_model.output])\n",
        "  # Provide a unique name for this Dense layer using tm in the name\n",
        "  final_dense = Dense(4, activation=\"relu\", name=f\"final_dense_{tm}\")(con_layer)\n",
        "  #dropout = Dropout(0.1)(final_dense)\n",
        "  # Provide a unique name for the output layer using tm in the name\n",
        "  out = Dense(3,activation='softmax', name=f\"output_layer_{tm}\")(final_dense)\n",
        "  com_model = Model(inputs = [visual_models_lsit[2].input, textual_model.input], outputs=out)\n",
        "\n",
        "  com_model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(), metrics = [\"accuracy\"])\n",
        "\n",
        "\n",
        "  print(\"\\nModel Name:=====>\\n\",visual_model_name[2]+'_'+dl_textual_model_name[tm]+'_late')\n",
        "\n",
        "  com_model.fit([train_image,train_pad_sequences],\n",
        "                      train_data['label'],\n",
        "                      validation_split = 0.1,\n",
        "                      epochs=20,\n",
        "                      batch_size=32,\n",
        "                      callbacks = callbacks_check(visual_model_name[2]+'_'+dl_textual_model_name[tm]+'_late')\n",
        "            )"
      ],
      "metadata": {
        "id": "KOKoGrlG7jC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inception"
      ],
      "metadata": {
        "id": "er1y3rKe7lag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for tm,textual_model in enumerate(dl_textual_models_lsit):\n",
        "  # Concatenating the output of 2 classifiers\n",
        "  # Reshape or flatten the output of one or both models to make them compatible for concatenation\n",
        "  visual_output = keras.layers.Flatten()(visual_models_lsit[3].output) # Flatten the visual model output\n",
        "  con_layer = keras.layers.concatenate([visual_output, textual_model.output])\n",
        "  # Provide a unique name for this Dense layer using tm in the name\n",
        "  final_dense = Dense(4, activation=\"relu\", name=f\"final_dense_{tm}\")(con_layer)\n",
        "  #dropout = Dropout(0.1)(final_dense)\n",
        "  # Provide a unique name for the output layer using tm in the name\n",
        "  out = Dense(3,activation='softmax', name=f\"output_layer_{tm}\")(final_dense)\n",
        "  com_model = Model(inputs = [visual_models_lsit[3].input, textual_model.input], outputs=out)\n",
        "\n",
        "  com_model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(), metrics = [\"accuracy\"])\n",
        "\n",
        "\n",
        "  print(\"\\nModel Name:=====>\\n\",visual_model_name[3]+'_'+dl_textual_model_name[tm]+'_late')\n",
        "\n",
        "  com_model.fit([train_image,train_pad_sequences],\n",
        "                      train_data['label'],\n",
        "                      validation_split = 0.1,\n",
        "                      epochs=20,\n",
        "                      batch_size=32,\n",
        "                      callbacks = callbacks_check(visual_model_name[3]+'_'+dl_textual_model_name[tm]+'_late')\n",
        "            )"
      ],
      "metadata": {
        "id": "xZbTs4Ad7mmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for vm in visual_model_name:\n",
        "  for tm in dl_textual_model_name:\n",
        "    print('Model name ===> ',vm+'_'+tm)\n",
        "    model = load_model(models_path+ vm+'_'+tm+'_late'+\".h5\")\n",
        "    pred = model.predict([test_image,test_pad_sequences])\n",
        "    y_pred = np.argmax(pred,axis=1)\n",
        "    print_metrices(test_data['label'], y_pred)"
      ],
      "metadata": {
        "id": "8i9H2wqE7q0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multimodal (Early Fusion)**"
      ],
      "metadata": {
        "id": "tcA10gcY7u0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visual_models():\n",
        "  '''\n",
        "      Input: Pretrained Models weight\n",
        "  '''\n",
        "  ## ResNet50\n",
        "  resnet = ResNet50(weights='imagenet', include_top=False,input_shape=(150, 150, 3))\n",
        "  resnet_model = resnet\n",
        "  resnet_model.trainable = False\n",
        "  resnet_y = resnet_model.output\n",
        "  resnet_pool = GlobalAveragePooling2D()(resnet_y)\n",
        "  resnet_output = Dense(10, activation='relu')(resnet_pool)\n",
        "  # this is the model we will train\n",
        "  resnet_img_model = Model(inputs=resnet_model.input, outputs=resnet_output)\n",
        "\n",
        "  ## Vgg16\n",
        "  vgg16 = VGG16(weights='imagenet', include_top=False,input_shape=(150, 150, 3))\n",
        "  vgg16_model = vgg16\n",
        "  vgg16_model.trainable = False\n",
        "  vgg16_y = vgg16_model.output\n",
        "  vgg16_pool = GlobalAveragePooling2D()(vgg16_y)\n",
        "  vgg16_output = Dense(10, activation='relu')(vgg16_pool)\n",
        "  # this is the model we will\n",
        "  vgg16_img_model = Model(inputs=vgg16_model.input, outputs=vgg16_output)\n",
        "\n",
        "  ## DenseNet\n",
        "  densenet = DenseNet121(weights='imagenet', include_top=False,input_shape=(150, 150, 3))\n",
        "  densenet_model = densenet\n",
        "  densenet_model.trainable = False\n",
        "  densenet_y = densenet_model.output\n",
        "  densenet_pool = GlobalAveragePooling2D()(densenet_y)\n",
        "  densenet_output = Dense(10, activation='relu')(densenet_pool)\n",
        "  # this is the model we will train\n",
        "  densenet_img_model = Model(inputs=densenet_model.input, outputs=densenet_output)\n",
        "\n",
        "  ## Inception v3\n",
        "  inception = InceptionV3(weights='imagenet', include_top=False,input_shape=(150, 150, 3))\n",
        "  inception_model = inception\n",
        "  inception_model.trainable = False\n",
        "  inception_y = inception_model.output\n",
        "  inception_pool = GlobalAveragePooling2D()(inception_y)\n",
        "  inception_output = Dense(10, activation='relu')(inception_pool)\n",
        "  # this is the model we will train\n",
        "  inception_img_model = Model(inputs=inception_model.input, outputs=inception_output)\n",
        "\n",
        "  #image_models = [inception_model, resnet_img_model,densenet_img_model, vgg16_img_model]\n",
        "  image_models = [resnet_img_model,densenet_img_model, vgg16_img_model,inception_model]\n",
        "\n",
        "  return image_models\n",
        "\n",
        "\n",
        "def textual_models():\n",
        "\n",
        "  ###### BiLSTM Model #######\n",
        "  bi_text_inputs = Input(shape=(max_len,))\n",
        "  bi_embedding_layer = Embedding(vocab_size,64)(bi_text_inputs)\n",
        "  LSTM_Layer_1 = Bidirectional(LSTM(32))(bi_embedding_layer)\n",
        "  bi_dense_layer_1 = Dense(10, activation='relu')(LSTM_Layer_1)\n",
        "  bilstm_model = Model(inputs=bi_text_inputs, outputs=bi_dense_layer_1)\n",
        "\n",
        "\n",
        "  ### CNN\n",
        "  cnn_text_inputs = Input(shape=(max_len,))\n",
        "  cnn_embedding_layer = Embedding(vocab_size,64)(cnn_text_inputs)\n",
        "  cnn_conv1 = Conv1D(64,2,activation='relu')(cnn_embedding_layer)\n",
        "  cnn_conv2 = Conv1D(32,2,activation='relu')(cnn_conv1)\n",
        "  cnn_pool1 = MaxPooling1D(2)(cnn_conv2)\n",
        "\n",
        "  cnn_flat = Flatten()(cnn_pool1)\n",
        "  cnn_dense_layer_1 = Dense(10, activation='relu')(cnn_flat)\n",
        "  cnn_model = Model(inputs=cnn_text_inputs, outputs=cnn_dense_layer_1)\n",
        "\n",
        "\n",
        "\n",
        "  ##### LSTM-CNN Model ####\n",
        "  lc_text_inputs = Input(shape=(max_len,))\n",
        "  lc_embedding_layer = Embedding(vocab_size,64)(lc_text_inputs)\n",
        "  LSTM_Layer = Bidirectional(LSTM(32,return_sequences=True))(lc_embedding_layer)\n",
        "  lc_conv1 = Conv1D(32,2,activation='relu')(LSTM_Layer)\n",
        "  lc_pool1 = MaxPooling1D(2)(lc_conv1)\n",
        "  lc_flat = Flatten()(lc_pool1)\n",
        "  lc_dense_layer_1 = Dense(10, activation='relu')(lc_flat)\n",
        "  cnn_lstm_model = Model(inputs=lc_text_inputs, outputs=lc_dense_layer_1)\n",
        "\n",
        "  ############\n",
        "  dl_textual_models = [bilstm_model,cnn_model,cnn_lstm_model]\n",
        "\n",
        "  return dl_textual_models"
      ],
      "metadata": {
        "id": "Rca3HTAWm-kI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visual_model_name = ['Inception', 'ResNet','DenseNet','Vgg16']\n",
        "visual_model_name = ['ResNet','DenseNet','Vgg16','Inception']\n",
        "visual_models_lsit = visual_models()\n",
        "\n",
        "dl_textual_model_name = ['LSTM','CNN','LSTM+CNN']\n",
        "dl_textual_models_lsit = textual_models()"
      ],
      "metadata": {
        "id": "M373a0rCnOYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for vm,visual_model in enumerate(visual_models_lsit):\n",
        "  for tm,textual_model in enumerate(dl_textual_models_lsit):\n",
        "    # Concatenating the output of 2 classifiers\n",
        "    # Flatten the visual model output to make it 2D\n",
        "    visual_flat = Flatten()(visual_model.output)\n",
        "    con_layer = keras.layers.concatenate([visual_flat, textual_model.output])\n",
        "\n",
        "    #final_dense = Dense(10, activation=\"relu\")(con_layer)\n",
        "    #dropout = Dropout(0.01)(final_dense)\n",
        "    out = Dense(2,activation='softmax')(con_layer)\n",
        "    com_model = Model(inputs = [visual_model.input, textual_model.input], outputs=out)\n",
        "\n",
        "    com_model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(), metrics = [\"accuracy\"])\n",
        "\n",
        "\n",
        "    print(\"\\nModel Name:=====>\\n\",visual_model_name[vm]+'_'+dl_textual_model_name[tm]+'_early')\n",
        "\n",
        "    com_model.fit([train_image,train_pad_sequences],\n",
        "                      train_data['label'],\n",
        "                      validation_split = 0.1,\n",
        "                      epochs=20,\n",
        "                      batch_size=32,\n",
        "                      callbacks = callbacks_check(visual_model_name[vm]+'_'+dl_textual_model_name[tm])\n",
        "            )"
      ],
      "metadata": {
        "id": "u-fcH3OgnRKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for vm in visual_model_name:\n",
        "  for tm in dl_textual_model_name:\n",
        "    print('Model name ===> ',vm+'_'+tm)\n",
        "    model = load_model(models_path+ vm+'_'+tm+\".h5\")\n",
        "    pred = model.predict([test_image,test_pad_sequences])\n",
        "    y_pred = np.argmax(pred,axis=1)\n",
        "    print_metrices(test_data['label'], y_pred)"
      ],
      "metadata": {
        "id": "exsUW9iAnSX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Proposed**"
      ],
      "metadata": {
        "id": "e7fnT3Ppnd2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (Input, GlobalAveragePooling2D, Dense, Flatten, Conv1D,\n",
        "                                     MaxPooling1D, LSTM, Bidirectional, Embedding, BatchNormalization,\n",
        "                                     Dropout, concatenate)\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg"
      ],
      "metadata": {
        "id": "ngjokXvDnf0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visual_models():\n",
        "    inception = InceptionV3(weights='imagenet', include_top=False, input_shape=(150, 150, 3))\n",
        "    inception_model = inception\n",
        "    inception_model.trainable = False\n",
        "    inception_y = inception_model.output\n",
        "    inception_pool = GlobalAveragePooling2D()(inception_y)\n",
        "    inception_bn = BatchNormalization()(inception_pool)\n",
        "    inception_dense1 = Dense(256, activation='relu')(inception_bn)\n",
        "    inception_dropout1 = Dropout(0.5)(inception_dense1)\n",
        "    inception_dense2 = Dense(128, activation='relu')(inception_dropout1)\n",
        "    inception_dropout2 = Dropout(0.5)(inception_dense2)\n",
        "    inception_output = Dense(10, activation='relu')(inception_dropout2)\n",
        "    inception_img_model = Model(inputs=inception_model.input, outputs=inception_output)\n",
        "    image_models = [inception_img_model]\n",
        "    return image_models\n",
        "\n",
        "\n",
        "def textual_models():\n",
        "    lc_text_inputs = Input(shape=(max_len,))\n",
        "    lc_embedding_layer = Embedding(vocab_size, 64)(lc_text_inputs)\n",
        "    LSTM_Layer = Bidirectional(LSTM(32, return_sequences=True))(lc_embedding_layer)\n",
        "    lc_conv1 = Conv1D(64, 2, activation='relu')(LSTM_Layer)\n",
        "    lc_bn1 = BatchNormalization()(lc_conv1)\n",
        "    lc_pool1 = MaxPooling1D(2)(lc_bn1)\n",
        "    lc_conv2 = Conv1D(64, 2, activation='relu')(lc_pool1)\n",
        "    lc_bn2 = BatchNormalization()(lc_conv2)\n",
        "    lc_pool2 = MaxPooling1D(2)(lc_bn2)\n",
        "    lc_flat = Flatten()(lc_pool2)\n",
        "    lc_dense_layer_1 = Dense(128, activation='relu')(lc_flat)\n",
        "    lc_dropout1 = Dropout(0.5)(lc_dense_layer_1)\n",
        "    lc_dense_layer_2 = Dense(10, activation='relu')(lc_dropout1)\n",
        "    cnn_lstm_model = Model(inputs=lc_text_inputs, outputs=lc_dense_layer_2)\n",
        "    dl_textual_models = [cnn_lstm_model]\n",
        "    return dl_textual_models"
      ],
      "metadata": {
        "id": "xcyC5hTZnkwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_model(model, model_name):\n",
        "    plot_model(model, to_file=f'{model_name}.png', show_shapes=True, show_layer_names=True)\n",
        "    img = mpimg.imread(f'{model_name}.png')\n",
        "    plt.figure(figsize=(100, 100))\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ixpNEZZmnlxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visual_model_name = ['InceptionV3']\n",
        "visual_models_lsit = visual_models()\n",
        "\n",
        "dl_textual_model_name = ['LSTM+CNN']\n",
        "dl_textual_models_lsit = textual_models()"
      ],
      "metadata": {
        "id": "H7kk_bRinnF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for vm, visual_model in enumerate(visual_models_lsit):\n",
        "    for tm, textual_model in enumerate(dl_textual_models_lsit):\n",
        "        con_layer = concatenate([visual_model.output, textual_model.output])\n",
        "        fusion_dense1 = Dense(256, activation='relu')(con_layer)\n",
        "        fusion_bn1 = BatchNormalization()(fusion_dense1)\n",
        "        fusion_dropout1 = Dropout(0.5)(fusion_bn1)\n",
        "        fusion_dense2 = Dense(128, activation='relu')(fusion_dropout1)\n",
        "        fusion_bn2 = BatchNormalization()(fusion_dense2)\n",
        "        fusion_dropout2 = Dropout(0.5)(fusion_bn2)\n",
        "        out = Dense(2, activation='softmax')(fusion_dropout2)\n",
        "        com_model = Model(inputs=[visual_model.input, textual_model.input], outputs=out)\n",
        "\n",
        "        com_model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(), metrics=[\"accuracy\"])\n",
        "\n",
        "        model_name = visual_model_name[vm] + '_proposed' + dl_textual_model_name[tm] + '_early'\n",
        "        print(\"\\nModel Name:=====>\\n\", model_name)\n",
        "\n",
        "        # print(\"\\nModel Name:=====>\\n\", visual_model_name[vm] + '_proposed' + dl_textual_model_name[tm] + '_early')\n",
        "\n",
        "        visualize_model(com_model, model_name)"
      ],
      "metadata": {
        "id": "mGY4qkSGnoum"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}